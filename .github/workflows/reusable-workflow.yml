name: Test Based on Environment

on:
  workflow_call:
    inputs:
      env:
        description: "The name of environment (eg. STAGING, PRODUCTION)"
        required: true
        type: string
      artifact-name:
        description: "The name of the artifact to download"
        required: true
        type: string
      test-type:
        description: "Type of tests to run (stability, smoke, full, performance)"
        required: false
        type: string
        default: "smoke"

jobs:
  test:
    strategy:
      matrix:
        browser: [ chrome ]
    runs-on: ubuntu-latest
    environment: ${{ inputs.env }}
    
    env:
      # Common environment variables used across all steps
      ENV_URL: ${{ vars.ENV_URL }}
      ENV_NAME: ${{ inputs.env }}
      OBI_USERNAME: ${{ secrets.OBI_USERNAME }}
      OBI_PASSWORD: ${{ secrets.OBI_PASSWORD }}
      LAB_ID_STAGING: ${{ secrets.LAB_ID_STAGING }}
      PROJECT_ID_STAGING: ${{ secrets.PROJECT_ID_STAGING }}
      LAB_ID_PRODUCTION: ${{ secrets.LAB_ID_PRODUCTION }}
      PROJECT_ID_PRODUCTION: ${{ secrets.PROJECT_ID_PRODUCTION }}
      DISPLAY: :99

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install required dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip wget xvfb

      # Installation of 'uv' is required
      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up virtual environment and install dependencies
        run: |
          uv venv -p 3.11
          python -m pip install --upgrade pip
          source .venv/bin/activate
          uv pip install -r requirements.txt
          ls -la

      - name: Verify Chrome installation
        run: google-chrome --version

      - name: Setup Xvfb for headless testing
        run: |
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

      # Create a directory for logs
      - name: Create Logs Directory
        run: mkdir -p latest_logs/errors

      - name: Run CI/CD Stability Tests First
        run: |
          echo "Running CI/CD stability tests first..."
          source .venv/bin/activate
          uv run pytest \
            tests/test_ci_cd_stability.py \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/stability_report_${ENV_NAME}_${{ matrix.browser }}.html" \
            --self-contained-html || echo "Stability tests completed with issues"

      - name: Run Test Suite
        run: |
          echo "RUN_URL=https://github.com/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" >> ${GITHUB_ENV}
          echo "Running ${{ inputs.test-type }} tests for ${{ matrix.browser }} in $ENV_URL with Username $OBI_USERNAME"
          echo "ENV_URL: $ENV_URL"
          echo "ENV_NAME: $ENV_NAME"
          echo "Browser: ${{ matrix.browser }}"
          echo "Test Type: ${{ inputs.test-type }}"

          source .venv/bin/activate
          set -o pipefail
          
          # Determine which tests to run based on test-type
          if [ "${{ inputs.test-type }}" = "stability" ]; then
            TEST_FILES="tests/test_ci_cd_stability.py"
            echo "Running CI/CD stability tests only"
          elif [ "${{ inputs.test-type }}" = "smoke" ]; then
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py tests/test_digital_brain_story.py tests/test_team.py tests/test_contact.py tests/test_explore_ephys.py"
            echo "Running smoke tests"
          elif [ "${{ inputs.test-type }}" = "performance" ]; then
            TEST_FILES="tests/test_performance_example.py"
            echo "Running performance tests"
          elif [ "${{ inputs.test-type }}" = "ai-assistant" ]; then
            TEST_FILES="tests/test_ai_assistant_workflow.py"
            echo "Running AI assistant tests only"
          else
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py tests/test_digital_brain_story.py tests/test_team.py tests/test_contact.py tests/test_explore_ephys.py"
            echo "Running full test suite"
          fi
          
          uv run pytest \
            $TEST_FILES \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/report_${ENV_NAME}_${{ matrix.browser }}_${{ inputs.test-type }}.html" \
            --self-contained-html \
            --maxfail=3 | tee test_output.log

      # Upload test artifacts (screenshots) if failure
      - name: Upload test artifacts (screenshots)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: error-screenshots-${{ matrix.browser }}-${{ github.run_id }}-${{ github.job }}-${{ github.run_attempt }}-${{ github.sha }}
          path: latest_logs/errors
          if-no-files-found: warn
          overwrite: true

      # Upload stability test report
      - name: Upload Stability Test Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stability-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/stability_report_${{ inputs.env }}_${{ matrix.browser }}.html
          if-no-files-found: warn

      # Run tests and generate HTML Report
      - name: Upload Pytest HTML Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: html-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/report_${{ inputs.env }}_${{ matrix.browser }}_${{ inputs.test-type }}.html
          if-no-files-found: warn

      # Upload performance reports if performance tests were run
      - name: Generate Performance HTML Reports
        if: always() && inputs.test-type == 'performance'
        run: |
          source .venv/bin/activate
          for json_file in performance_*.json; do
            if [ -f "$json_file" ]; then
              echo "Generating HTML report for $json_file"
              python generate_performance_html.py "$json_file"
            fi
          done

      - name: Upload Performance Reports
        if: always() && inputs.test-type == 'performance'
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: |
            performance_*.json
            performance_*.html
          if-no-files-found: warn

      - name: Create and Upload Failure Summary
        if: failure()
        run: |
          # Create a failure summary file
          FAILURE_FILE="failures_${{ inputs.env }}_${{ matrix.browser }}.md"
          echo "# Test Failures Summary" > "$FAILURE_FILE"
          echo "" >> "$FAILURE_FILE"
          echo "**Environment:** ${{ inputs.env }}" >> "$FAILURE_FILE"
          echo "**Browser:** ${{ matrix.browser }}" >> "$FAILURE_FILE"
          echo "**Test Type:** ${{ inputs.test-type }}" >> "$FAILURE_FILE"
          echo "**Run ID:** ${{ github.run_id }}" >> "$FAILURE_FILE"
          echo "" >> "$FAILURE_FILE"
          
          if [ -f test_output.log ]; then
            echo "## Failed Tests" >> "$FAILURE_FILE"
            FAILED_TESTS=$(sed -n '/=* FAILURES =*/,/short test summary info/p' test_output.log | grep -E '^FAILED tests/' | head -5)
            if [ -n "$FAILED_TESTS" ]; then
              echo '```' >> "$FAILURE_FILE"
              echo "$FAILED_TESTS" >> "$FAILURE_FILE"
              echo '```' >> "$FAILURE_FILE"
              
              # Also create annotation
              FAILED_LIST=$(echo "$FAILED_TESTS" | sed 's/FAILED tests\///' | sed 's/::.*$//' | tr '\n' ', ' | sed 's/, $//')
              echo "::error title=Test Failures (${{ inputs.test-type }} suite)::$FAILED_LIST"
            else
              echo "No specific test failures found in logs" >> "$FAILURE_FILE"
              echo "::error title=Test Failures (${{ inputs.test-type }} suite)::Some tests failed - check logs for details"
            fi
          else
            echo "No test output log found" >> "$FAILURE_FILE"
            echo "::error title=Test Failures (${{ inputs.test-type }} suite)::Tests failed but no output log available"
          fi

      - name: Upload Failure Summary
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-failures-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: failures_${{ inputs.env }}_${{ matrix.browser }}.md
          if-no-files-found: warn

      # Parse test results and create Teams notification
      - name: Parse Test Results and Create Teams Notification
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || failure())
        run: |
          echo "Parsing test results for Teams notification..."
          
          # Initialize variables
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0
          SKIPPED_TESTS=0
          FAILED_TEST_LIST=""
          TEST_DURATION="Unknown"
          
          # Parse pytest output if available
          if [ -f test_output.log ]; then
            echo "Found test output log, parsing results..."
            
            # Extract test summary line (e.g., "= 5 failed, 10 passed, 2 skipped in 45.67s =")
            SUMMARY_LINE=$(grep -E "=.*failed.*passed.*in.*s.*=" test_output.log | tail -1)
            
            if [ -n "$SUMMARY_LINE" ]; then
              echo "Summary line found: $SUMMARY_LINE"
              
              # Extract numbers using sed/grep
              FAILED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.*= \([0-9]*\) failed.*/\1/p')
              PASSED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.* \([0-9]*\) passed.*/\1/p')
              SKIPPED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.* \([0-9]*\) skipped.*/\1/p')
              TEST_DURATION=$(echo "$SUMMARY_LINE" | sed -n 's/.* in \([0-9]*\.[0-9]*s\).*/\1/p')
              
              # Handle cases where some numbers might be missing
              FAILED_TESTS=${FAILED_TESTS:-0}
              PASSED_TESTS=${PASSED_TESTS:-0}
              SKIPPED_TESTS=${SKIPPED_TESTS:-0}
              
              TOTAL_TESTS=$((FAILED_TESTS + PASSED_TESTS + SKIPPED_TESTS))
            else
              # Try alternative parsing for different pytest output formats
              FAILED_TESTS=$(grep -o "[0-9]* failed" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              PASSED_TESTS=$(grep -o "[0-9]* passed" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              SKIPPED_TESTS=$(grep -o "[0-9]* skipped" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              TEST_DURATION=$(grep -o "in [0-9]*\.[0-9]*s" test_output.log | head -1 | sed 's/in //' || echo "Unknown")
              TOTAL_TESTS=$((FAILED_TESTS + PASSED_TESTS + SKIPPED_TESTS))
            fi
            
            # Extract failed test names and create clickable links (limit to first 5)
            FAILED_TEST_LIST=""
            FAILED_TEST_LINKS=""
            if [ -f test_output.log ]; then
              FAILED_TESTS_RAW=$(grep -E "^FAILED tests/" test_output.log | head -5 | sed 's/FAILED tests\///')
              
              if [ -n "$FAILED_TESTS_RAW" ]; then
                # Create comma-separated list for summary
                FAILED_TEST_LIST=$(echo "$FAILED_TESTS_RAW" | sed 's/::.*$//' | tr '\n' ', ' | sed 's/, $//')
                
                # Create JSON array of failed test links for Teams card
                FAILED_TEST_LINKS="["
                FIRST=true
                while IFS= read -r line; do
                  TEST_FILE=$(echo "$line" | sed 's/::.*$//')
                  TEST_NAME=$(echo "$line" | sed 's/^.*:://')
                  
                  if [ "$FIRST" = true ]; then
                    FIRST=false
                  else
                    FAILED_TEST_LINKS="$FAILED_TEST_LINKS,"
                  fi
                  
                  FAILED_TEST_LINKS="$FAILED_TEST_LINKS{\"type\":\"Action.OpenUrl\",\"title\":\"‚ùå $TEST_FILE\",\"url\":\"https://github.com/${{ github.repository }}/blob/${{ github.ref_name }}/$TEST_FILE\"}"
                done <<< "$FAILED_TESTS_RAW"
                FAILED_TEST_LINKS="$FAILED_TEST_LINKS]"
              fi
            fi
          fi
          
          # Determine overall status
          if [ "$FAILED_TESTS" -gt 0 ]; then
            OVERALL_STATUS="FAILED"
            STATUS_COLOR="attention"
            STATUS_EMOJI="‚ùå"
          elif [ "$TOTAL_TESTS" -gt 0 ]; then
            OVERALL_STATUS="PASSED"
            STATUS_COLOR="good"
            STATUS_EMOJI="‚úÖ"
          else
            OVERALL_STATUS="NO_TESTS"
            STATUS_COLOR="warning"
            STATUS_EMOJI="‚ö†Ô∏è"
          fi
          
          # Get commit info
          COMMIT_SHA=$(echo "$GITHUB_SHA" | cut -c1-7)
          COMMIT_MESSAGE=$(git log -1 --pretty=format:"%s" 2>/dev/null || echo "Unknown commit")
          RUN_URL="https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID"
          
          # Create environment variables for Teams notification
          echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
          echo "PASSED_TESTS=$PASSED_TESTS" >> $GITHUB_ENV
          echo "FAILED_TESTS=$FAILED_TESTS" >> $GITHUB_ENV
          echo "SKIPPED_TESTS=$SKIPPED_TESTS" >> $GITHUB_ENV
          echo "FAILED_TEST_LIST=$FAILED_TEST_LIST" >> $GITHUB_ENV
          echo "FAILED_TEST_LINKS<<EOF" >> $GITHUB_ENV
          echo "$FAILED_TEST_LINKS" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
          echo "TEST_DURATION=$TEST_DURATION" >> $GITHUB_ENV
          echo "OVERALL_STATUS=$OVERALL_STATUS" >> $GITHUB_ENV
          echo "STATUS_COLOR=$STATUS_COLOR" >> $GITHUB_ENV
          echo "STATUS_EMOJI=$STATUS_EMOJI" >> $GITHUB_ENV
          echo "COMMIT_SHA=$COMMIT_SHA" >> $GITHUB_ENV
          echo "COMMIT_MESSAGE=$COMMIT_MESSAGE" >> $GITHUB_ENV
          echo "RUN_URL=$RUN_URL" >> $GITHUB_ENV
          
          # Debug output
          echo "Test Summary:"
          echo "  Total: $TOTAL_TESTS"
          echo "  Passed: $PASSED_TESTS"
          echo "  Failed: $FAILED_TESTS"
          echo "  Skipped: $SKIPPED_TESTS"
          echo "  Duration: $TEST_DURATION"
          echo "  Status: $OVERALL_STATUS"
          echo "  Failed Tests: $FAILED_TEST_LIST"
      
      # Build Teams notification payload with dynamic failed test links
      - name: Build Teams Notification Payload
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || failure())
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import re
          
          # Read environment variables
          total_tests = os.getenv('TOTAL_TESTS', '0')
          passed_tests = os.getenv('PASSED_TESTS', '0')
          failed_tests = os.getenv('FAILED_TESTS', '0')
          skipped_tests = os.getenv('SKIPPED_TESTS', '0')
          test_duration = os.getenv('TEST_DURATION', 'Unknown')
          overall_status = os.getenv('OVERALL_STATUS', 'UNKNOWN')
          status_emoji = os.getenv('STATUS_EMOJI', '‚ùì')
          commit_sha = os.getenv('COMMIT_SHA', 'unknown')
          commit_message = os.getenv('COMMIT_MESSAGE', 'Unknown commit')
          
          # Build base actions
          actions = [
              {
                  "type": "Action.OpenUrl",
                  "title": "üîç View Workflow Run",
                  "url": f"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
              },
              {
                  "type": "Action.OpenUrl",
                  "title": "üìã View Test Plan",
                  "url": "https://github.com/${{ github.repository }}/blob/main/Virtual_Lab_Test_Plan.xlsx"
              }
          ]
          
          # Add failed test links if there are failures
          if int(failed_tests) > 0:
              try:
                  with open('test_output.log', 'r') as f:
                      content = f.read()
                      failed_pattern = r'FAILED tests/(.+?)(?=\s|$)'
                      matches = re.findall(failed_pattern, content)
                      
                      for match in matches[:5]:  # Limit to 5 failed tests
                          test_file = match.split('::')[0]
                          actions.append({
                              "type": "Action.OpenUrl",
                              "title": f"‚ùå {test_file}",
                              "url": f"https://github.com/${{ github.repository }}/blob/${{ github.ref_name }}/tests/{test_file}"
                          })
              except Exception as e:
                  print(f"Error reading failed tests: {e}")
          
          # Build complete payload
          payload = {
              "type": "message",
              "attachments": [
                  {
                      "contentType": "application/vnd.microsoft.card.adaptive",
                      "contentUrl": None,
                      "content": {
                          "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                          "type": "AdaptiveCard",
                          "version": "1.4",
                          "body": [
                              {
                                  "type": "TextBlock",
                                  "text": f"{status_emoji} Virtual Lab Test Results - {overall_status}",
                                  "weight": "bolder",
                                  "size": "large",
                                  "color": "attention" if overall_status == "FAILED" else "good"
                              },
                              {
                                  "type": "FactSet",
                                  "facts": [
                                      {"title": "Environment:", "value": "${{ inputs.env }}"},
                                      {"title": "Test Suite:", "value": "${{ inputs.test-type }}"},
                                      {"title": "Browser:", "value": "${{ matrix.browser }}"},
                                      {"title": "Branch:", "value": "${{ github.ref_name }}"},
                                      {"title": "Commit:", "value": f"{commit_sha} - {commit_message}"},
                                      {"title": "Duration:", "value": test_duration}
                                  ]
                              },
                              {
                                  "type": "Container",
                                  "style": "emphasis",
                                  "items": [
                                      {
                                          "type": "TextBlock",
                                          "text": "üìä Test Summary",
                                          "weight": "bolder"
                                      },
                                      {
                                          "type": "ColumnSet",
                                          "columns": [
                                              {
                                                  "type": "Column",
                                                  "width": "stretch",
                                                  "items": [
                                                      {
                                                          "type": "TextBlock",
                                                          "text": f"**Total:** {total_tests}",
                                                          "wrap": True
                                                      }
                                                  ]
                                              },
                                              {
                                                  "type": "Column",
                                                  "width": "stretch",
                                                  "items": [
                                                      {
                                                          "type": "TextBlock",
                                                          "text": f"**‚úÖ Passed:** {passed_tests}",
                                                          "wrap": True,
                                                          "color": "good"
                                                      }
                                                  ]
                                              },
                                              {
                                                  "type": "Column",
                                                  "width": "stretch",
                                                  "items": [
                                                      {
                                                          "type": "TextBlock",
                                                          "text": f"**‚ùå Failed:** {failed_tests}",
                                                          "wrap": True,
                                                          "color": "attention" if int(failed_tests) > 0 else "default"
                                                      }
                                                  ]
                                              },
                                              {
                                                  "type": "Column",
                                                  "width": "stretch",
                                                  "items": [
                                                      {
                                                          "type": "TextBlock",
                                                          "text": f"**‚è≠Ô∏è Skipped:** {skipped_tests}",
                                                          "wrap": True
                                                      }
                                                  ]
                                              }
                                          ]
                                      }
                                  ]
                              }
                          ],
                          "actions": actions
                      }
                  }
              ]
          }
          
          # Add failed tests section if there are failures
          if int(failed_tests) > 0:
              payload["attachments"][0]["content"]["body"].append({
                  "type": "Container",
                  "style": "attention",
                  "items": [
                      {
                          "type": "TextBlock",
                          "text": "üö® Failed Tests (click links below to view)",
                          "weight": "bolder",
                          "color": "attention"
                      }
                  ]
              })
          
          # Save payload to file
          with open('teams_payload.json', 'w') as f:
              json.dump(payload, f, indent=2)
          
          print("Teams payload generated successfully")
          PYTHON_SCRIPT
          
          # Read the payload and set as output
          PAYLOAD=$(cat teams_payload.json)
          echo "TEAMS_PAYLOAD<<EOF" >> $GITHUB_ENV
          echo "$PAYLOAD" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
          
      # Send enhanced MS Teams notification with proper card format
      - name: Send Teams Notification
        uses: skitionek/notify-microsoft-teams@v1.0.9
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || failure())
        with:
          webhook_url: ${{ secrets.MS_TEAMS_NEW_WEBHOOK_URI }}
          raw: ${{ env.TEAMS_PAYLOAD }}
