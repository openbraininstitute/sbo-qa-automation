name: Test Based on Environment

on:
  workflow_call:
    inputs:
      env:
        description: "The name of environment (eg. STAGING, PRODUCTION)"
        required: true
        type: string
      artifact-name:
        description: "The name of the artifact to download"
        required: true
        type: string
      test-type:
        description: "Type of tests to run (stability, smoke, full, performance)"
        required: false
        type: string
        default: "smoke"

jobs:
  test:
    strategy:
      matrix:
        browser: [ chrome ]
    runs-on: ubuntu-latest
    environment: ${{ inputs.env }}
    
    env:
      # Common environment variables used across all steps
      ENV_URL: ${{ vars.ENV_URL }}
      ENV_NAME: ${{ inputs.env }}
      OBI_USERNAME: ${{ secrets.OBI_USERNAME }}
      OBI_PASSWORD: ${{ secrets.OBI_PASSWORD }}
      LAB_ID_STAGING: ${{ secrets.LAB_ID_STAGING }}
      PROJECT_ID_STAGING: ${{ secrets.PROJECT_ID_STAGING }}
      LAB_ID_PRODUCTION: ${{ secrets.LAB_ID_PRODUCTION }}
      PROJECT_ID_PRODUCTION: ${{ secrets.PROJECT_ID_PRODUCTION }}
      DISPLAY: :99

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install required dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip wget xvfb

      # Installation of 'uv' is required
      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up virtual environment and install dependencies
        run: |
          uv venv -p 3.11
          python -m pip install --upgrade pip
          source .venv/bin/activate
          uv pip install -r requirements.txt
          ls -la

      - name: Verify Chrome installation
        run: google-chrome --version

      - name: Setup Xvfb for headless testing
        run: |
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

      # Create a directory for logs
      - name: Create Logs Directory
        run: mkdir -p latest_logs/errors

      - name: Run CI/CD Stability Tests First
        run: |
          echo "Running CI/CD stability tests first..."
          source .venv/bin/activate
          uv run pytest \
            tests/test_ci_cd_stability.py \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/stability_report_${ENV_NAME}_${{ matrix.browser }}.html" \
            --self-contained-html || echo "Stability tests completed with issues"

      - name: Run Test Suite
        run: |
          echo "RUN_URL=https://github.com/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" >> ${GITHUB_ENV}
          echo "Running ${{ inputs.test-type }} tests for ${{ matrix.browser }} in $ENV_URL with Username $OBI_USERNAME"
          echo "ENV_URL: $ENV_URL"
          echo "ENV_NAME: $ENV_NAME"
          echo "Browser: ${{ matrix.browser }}"
          echo "Test Type: ${{ inputs.test-type }}"

          source .venv/bin/activate
          set -o pipefail
          
          # Determine which tests to run based on test-type
          if [ "${{ inputs.test-type }}" = "stability" ]; then
            TEST_FILES="tests/test_ci_cd_stability.py"
            echo "Running CI/CD stability tests only"
          elif [ "${{ inputs.test-type }}" = "smoke" ]; then
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py"
            echo "Running smoke tests"
          elif [ "${{ inputs.test-type }}" = "performance" ]; then
            TEST_FILES="tests/test_performance_example.py"
            echo "Running performance tests"
          elif [ "${{ inputs.test-type }}" = "ai-assistant" ]; then
            TEST_FILES="tests/test_ai_assistant_workflow.py"
            echo "Running AI assistant tests only"
          else
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py"
            echo "Running full test suite"
          fi
          
          uv run pytest \
            $TEST_FILES \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/report_${ENV_NAME}_${{ matrix.browser }}_${{ inputs.test-type }}.html" \
            --self-contained-html \
            --maxfail=3 | tee test_output.log

      # Upload test artifacts (screenshots) if failure
      - name: Upload test artifacts (screenshots)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: error-screenshots-${{ matrix.browser }}-${{ github.run_id }}-${{ github.job }}-${{ github.run_attempt }}-${{ github.sha }}
          path: latest_logs/errors
          if-no-files-found: warn
          overwrite: true

      # Upload stability test report
      - name: Upload Stability Test Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stability-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/stability_report_${{ inputs.env }}_${{ matrix.browser }}.html
          if-no-files-found: warn

      # Run tests and generate HTML Report
      - name: Upload Pytest HTML Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: html-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/report_${{ inputs.env }}_${{ matrix.browser }}_${{ inputs.test-type }}.html
          if-no-files-found: warn

      # Upload performance reports if performance tests were run
      - name: Generate Performance HTML Reports
        if: always() && inputs.test-type == 'performance'
        run: |
          source .venv/bin/activate
          for json_file in performance_*.json; do
            if [ -f "$json_file" ]; then
              echo "Generating HTML report for $json_file"
              python generate_performance_html.py "$json_file"
            fi
          done

      - name: Upload Performance Reports
        if: always() && inputs.test-type == 'performance'
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: |
            performance_*.json
            performance_*.html
          if-no-files-found: warn

      - name: Annotation - Test Summary - Failures
        if: failure()
        run: |
          # Create a simple failure summary
          FAILED_TESTS=""
          if [ -f test_output.log ]; then
            FAILED_TESTS=$(sed -n '/=* FAILURES =*/,/short test summary info/p' test_output.log | grep -E '^- tests/' | head -3 | tr '\n' ' ')
          fi
          
          if [ -n "$FAILED_TESTS" ]; then
            echo "::error title=Test Failures (${{ inputs.test-type }} suite)::$FAILED_TESTS"
          else
            echo "::error title=Test Failures (${{ inputs.test-type }} suite)::Some tests failed - check logs for details"
          fi

      - name: Upload Failure Summary
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-failures-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: failures_${{ inputs.env }}_${{ matrix.browser }}.md
          if-no-files-found: warn

      # MS Teams notification - only for scheduled test runs (daily/weekly), not on git push
      - name: MS Teams Notification
        uses: skitionek/notify-microsoft-teams@v1.0.9
        if: failure() && github.event_name == 'schedule'
        with:
          webhook_url: ${{ secrets.MS_TEAMS_NEW_WEBHOOK_URI }}
          job: ${{ toJson(job) }}
          steps: ${{ toJson(steps) }}
          raw: >-
            {"type": "message","attachments": [{"contentType": "application/vnd.microsoft.card.adaptive","content": {"type": "AdaptiveCard","msteams": {"width": "Full"},"body": [{"type": "Container","items": [{"type": "RichTextBlock","inlines": [{"type": "TextRun","text": "Attention! Some UI tests failed, check the workflow for details.","color": "attention","size": "medium","weight": "bolder"}]}]}],"actions": [{"type": "Action.OpenUrl","title": "View Workflow Run","url": "${{ env.RUN_URL }}"}]}}]}
