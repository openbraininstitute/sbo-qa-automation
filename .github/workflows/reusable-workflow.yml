name: Test Based on Environment

on:
  workflow_call:
    inputs:
      env:
        description: "The name of environment (eg. STAGING, PRODUCTION)"
        required: true
        type: string
      artifact-name:
        description: "The name of the artifact to download"
        required: true
        type: string
      test-type:
        description: "Type of tests to run (stability, smoke, full, performance)"
        required: false
        type: string
        default: "smoke"

jobs:
  test:
    strategy:
      matrix:
        browser: [ chrome ]
    runs-on: ubuntu-latest
    environment: ${{ inputs.env }}
    
    env:
      # Common environment variables used across all steps
      ENV_URL: ${{ vars.ENV_URL }}
      ENV_NAME: ${{ inputs.env }}
      OBI_USERNAME: ${{ secrets.OBI_USERNAME }}
      OBI_PASSWORD: ${{ secrets.OBI_PASSWORD }}
      LAB_ID_STAGING: ${{ secrets.LAB_ID_STAGING }}
      PROJECT_ID_STAGING: ${{ secrets.PROJECT_ID_STAGING }}
      LAB_ID_PRODUCTION: ${{ secrets.LAB_ID_PRODUCTION }}
      PROJECT_ID_PRODUCTION: ${{ secrets.PROJECT_ID_PRODUCTION }}
      DISPLAY: :99

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install required dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip wget xvfb

      # Installation of 'uv' is required
      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up virtual environment and install dependencies
        run: |
          uv venv -p 3.11
          python -m pip install --upgrade pip
          source .venv/bin/activate
          uv pip install -r requirements.txt
          ls -la

      - name: Verify Chrome installation
        run: google-chrome --version

      - name: Setup Xvfb for headless testing
        run: |
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

      # Create a directory for logs
      - name: Create Logs Directory
        run: mkdir -p latest_logs/errors

      - name: Run CI/CD Stability Tests First
        run: |
          echo "Running CI/CD stability tests first..."
          source .venv/bin/activate
          uv run pytest \
            tests/test_ci_cd_stability.py \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/stability_report_${ENV_NAME}_${{ matrix.browser }}.html" \
            --self-contained-html || echo "Stability tests completed with issues"

      - name: Run Test Suite
        run: |
          echo "RUN_URL=https://github.com/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" >> ${GITHUB_ENV}
          echo "Running ${{ inputs.test-type }} tests for ${{ matrix.browser }} in $ENV_URL with Username $OBI_USERNAME"
          echo "ENV_URL: $ENV_URL"
          echo "ENV_NAME: $ENV_NAME"
          echo "Browser: ${{ matrix.browser }}"
          echo "Test Type: ${{ inputs.test-type }}"

          source .venv/bin/activate
          set -o pipefail
          
          # Determine which tests to run based on test-type
          if [ "${{ inputs.test-type }}" = "stability" ]; then
            TEST_FILES="tests/test_ci_cd_stability.py"
            echo "Running CI/CD stability tests only"
          elif [ "${{ inputs.test-type }}" = "smoke" ]; then
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py"
            echo "Running smoke tests"
          elif [ "${{ inputs.test-type }}" = "performance" ]; then
            TEST_FILES="tests/test_performance_example.py"
            echo "Running performance tests"
          elif [ "${{ inputs.test-type }}" = "ai-assistant" ]; then
            TEST_FILES="tests/test_ai_assistant_workflow.py"
            echo "Running AI assistant tests only"
          else
            TEST_FILES="tests/test_about.py tests/test_mission.py tests/test_news.py tests/test_landing.py tests/test_login.py tests/test_project_home.py tests/test_project_notebooks.py tests/test_explore_page.py tests/test_explore_emodel.py tests/test_ai_assistant_workflow.py tests/test_build_single_neuron.py tests/test_build_synaptome.py"
            echo "Running full test suite"
          fi
          
          uv run pytest \
            $TEST_FILES \
            --env="$ENV_NAME" \
            --env_url="$ENV_URL" \
            --browser-name=${{ matrix.browser }} \
            --headless \
            -v \
            --tb=short \
            --html="latest_logs/report_${ENV_NAME}_${{ matrix.browser }}_${{ inputs.test-type }}.html" \
            --self-contained-html \
            --maxfail=3 | tee test_output.log

      # Upload test artifacts (screenshots) if failure
      - name: Upload test artifacts (screenshots)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: error-screenshots-${{ matrix.browser }}-${{ github.run_id }}-${{ github.job }}-${{ github.run_attempt }}-${{ github.sha }}
          path: latest_logs/errors
          if-no-files-found: warn
          overwrite: true

      # Upload stability test report
      - name: Upload Stability Test Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stability-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/stability_report_${{ inputs.env }}_${{ matrix.browser }}.html
          if-no-files-found: warn

      # Run tests and generate HTML Report
      - name: Upload Pytest HTML Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: html-report-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: latest_logs/report_${{ inputs.env }}_${{ matrix.browser }}_${{ inputs.test-type }}.html
          if-no-files-found: warn

      # Upload performance reports if performance tests were run
      - name: Generate Performance HTML Reports
        if: always() && inputs.test-type == 'performance'
        run: |
          source .venv/bin/activate
          for json_file in performance_*.json; do
            if [ -f "$json_file" ]; then
              echo "Generating HTML report for $json_file"
              python generate_performance_html.py "$json_file"
            fi
          done

      - name: Upload Performance Reports
        if: always() && inputs.test-type == 'performance'
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: |
            performance_*.json
            performance_*.html
          if-no-files-found: warn

      - name: Annotation - Test Summary - Failures
        if: failure()
        run: |
          # Create a simple failure summary
          FAILED_TESTS=""
          if [ -f test_output.log ]; then
            FAILED_TESTS=$(sed -n '/=* FAILURES =*/,/short test summary info/p' test_output.log | grep -E '^- tests/' | head -3 | tr '\n' ' ')
          fi
          
          if [ -n "$FAILED_TESTS" ]; then
            echo "::error title=Test Failures (${{ inputs.test-type }} suite)::$FAILED_TESTS"
          else
            echo "::error title=Test Failures (${{ inputs.test-type }} suite)::Some tests failed - check logs for details"
          fi

      - name: Upload Failure Summary
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-failures-${{ inputs.test-type }}-${{ inputs.env }}-${{ matrix.browser }}-${{ github.run_id }}
          path: failures_${{ inputs.env }}_${{ matrix.browser }}.md
          if-no-files-found: warn

      # Parse test results and create Teams notification
      - name: Parse Test Results and Create Teams Notification
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || failure())
        run: |
          echo "Parsing test results for Teams notification..."
          
          # Initialize variables
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0
          SKIPPED_TESTS=0
          FAILED_TEST_LIST=""
          TEST_DURATION="Unknown"
          
          # Parse pytest output if available
          if [ -f test_output.log ]; then
            echo "Found test output log, parsing results..."
            
            # Extract test summary line (e.g., "= 5 failed, 10 passed, 2 skipped in 45.67s =")
            SUMMARY_LINE=$(grep -E "=.*failed.*passed.*in.*s.*=" test_output.log | tail -1)
            
            if [ -n "$SUMMARY_LINE" ]; then
              echo "Summary line found: $SUMMARY_LINE"
              
              # Extract numbers using sed/grep
              FAILED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.*= \([0-9]*\) failed.*/\1/p')
              PASSED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.* \([0-9]*\) passed.*/\1/p')
              SKIPPED_TESTS=$(echo "$SUMMARY_LINE" | sed -n 's/.* \([0-9]*\) skipped.*/\1/p')
              TEST_DURATION=$(echo "$SUMMARY_LINE" | sed -n 's/.* in \([0-9]*\.[0-9]*s\).*/\1/p')
              
              # Handle cases where some numbers might be missing
              FAILED_TESTS=${FAILED_TESTS:-0}
              PASSED_TESTS=${PASSED_TESTS:-0}
              SKIPPED_TESTS=${SKIPPED_TESTS:-0}
              
              TOTAL_TESTS=$((FAILED_TESTS + PASSED_TESTS + SKIPPED_TESTS))
            else
              # Try alternative parsing for different pytest output formats
              FAILED_TESTS=$(grep -o "[0-9]* failed" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              PASSED_TESTS=$(grep -o "[0-9]* passed" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              SKIPPED_TESTS=$(grep -o "[0-9]* skipped" test_output.log | head -1 | grep -o "[0-9]*" || echo "0")
              TEST_DURATION=$(grep -o "in [0-9]*\.[0-9]*s" test_output.log | head -1 | sed 's/in //' || echo "Unknown")
              TOTAL_TESTS=$((FAILED_TESTS + PASSED_TESTS + SKIPPED_TESTS))
            fi
            
            # Extract failed test names (limit to first 3 for readability)
            FAILED_TEST_LIST=$(grep -E "^FAILED tests/" test_output.log | head -3 | sed 's/FAILED tests\///' | sed 's/::.*$//' | tr '\n' ', ' | sed 's/, $//')
          fi
          
          # Determine overall status
          if [ "$FAILED_TESTS" -gt 0 ]; then
            OVERALL_STATUS="FAILED"
            STATUS_COLOR="attention"
            STATUS_EMOJI="‚ùå"
          elif [ "$TOTAL_TESTS" -gt 0 ]; then
            OVERALL_STATUS="PASSED"
            STATUS_COLOR="good"
            STATUS_EMOJI="‚úÖ"
          else
            OVERALL_STATUS="NO_TESTS"
            STATUS_COLOR="warning"
            STATUS_EMOJI="‚ö†Ô∏è"
          fi
          
          # Get commit info
          COMMIT_SHA=$(echo "$GITHUB_SHA" | cut -c1-7)
          COMMIT_MESSAGE=$(git log -1 --pretty=format:"%s" 2>/dev/null || echo "Unknown commit")
          
          # Create environment variables for Teams notification
          echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
          echo "PASSED_TESTS=$PASSED_TESTS" >> $GITHUB_ENV
          echo "FAILED_TESTS=$FAILED_TESTS" >> $GITHUB_ENV
          echo "SKIPPED_TESTS=$SKIPPED_TESTS" >> $GITHUB_ENV
          echo "FAILED_TEST_LIST=$FAILED_TEST_LIST" >> $GITHUB_ENV
          echo "TEST_DURATION=$TEST_DURATION" >> $GITHUB_ENV
          echo "OVERALL_STATUS=$OVERALL_STATUS" >> $GITHUB_ENV
          echo "STATUS_COLOR=$STATUS_COLOR" >> $GITHUB_ENV
          echo "STATUS_EMOJI=$STATUS_EMOJI" >> $GITHUB_ENV
          echo "COMMIT_SHA=$COMMIT_SHA" >> $GITHUB_ENV
          echo "COMMIT_MESSAGE=$COMMIT_MESSAGE" >> $GITHUB_ENV
          
          # Debug output
          echo "Test Summary:"
          echo "  Total: $TOTAL_TESTS"
          echo "  Passed: $PASSED_TESTS"
          echo "  Failed: $FAILED_TESTS"
          echo "  Skipped: $SKIPPED_TESTS"
          echo "  Duration: $TEST_DURATION"
          echo "  Status: $OVERALL_STATUS"
          echo "  Failed Tests: $FAILED_TEST_LIST"
          
      # Send enhanced MS Teams notification with proper card format
      - name: Send Teams Notification
        uses: skitionek/notify-microsoft-teams@v1.0.9
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || failure())
        with:
          webhook_url: ${{ secrets.MS_TEAMS_NEW_WEBHOOK_URI }}
          title: "${{ env.STATUS_EMOJI }} Virtual Lab Test Results - ${{ env.OVERALL_STATUS }}"
          summary: "${{ inputs.test-type }} tests on ${{ inputs.env }} environment"
          text: |
            **üìä Test Summary**
            - **Total Tests:** ${{ env.TOTAL_TESTS }}
            - **‚úÖ Passed:** ${{ env.PASSED_TESTS }}
            - **‚ùå Failed:** ${{ env.FAILED_TESTS }}
            - **‚è≠Ô∏è Skipped:** ${{ env.SKIPPED_TESTS }}
            - **‚è±Ô∏è Duration:** ${{ env.TEST_DURATION }}
            
            **üìã Environment Details**
            - **Environment:** ${{ inputs.env }}
            - **Test Suite:** ${{ inputs.test-type }}
            - **Browser:** ${{ matrix.browser }}
            - **Branch:** ${{ github.ref_name }}
            - **Commit:** ${{ env.COMMIT_SHA }} - ${{ env.COMMIT_MESSAGE }}
            
            ${{ env.FAILED_TESTS != '0' && format('**üö® Failed Tests:** {0}', env.FAILED_TEST_LIST) || '' }}
            
            **üîó Quick Links**
            - [üîç View Workflow Run](${{ env.RUN_URL }})
            - [üìä Download HTML Report](${{ env.RUN_URL }})
            ${{ env.FAILED_TESTS != '0' && format('- [üñºÔ∏è View Screenshots]({0})', env.RUN_URL) || '' }}
            - [üìã View Test Plan](https://github.com/${{ github.repository }}/blob/main/Virtual_Lab_Test_Plan.xlsx)
          theme_color: ${{ env.OVERALL_STATUS == 'FAILED' && 'FF0000' || env.OVERALL_STATUS == 'PASSED' && '00FF00' || 'FFA500' }}
